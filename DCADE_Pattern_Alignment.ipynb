{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from read_file import read_file\n",
    "import node_op\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import GBM\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_encode = 1\n",
    "brute = 1\n",
    "model_predict = 1\n",
    "train = 0\n",
    "drop_last = 1\n",
    "seg_method = 1\n",
    "MINIMAL_REPEAT = 5\n",
    "IGNORE_LEN = 5\n",
    "txt = 0\n",
    "current_path = os.path.join(\".\", \"websites\", \"N7\")\n",
    "#---------------------\n",
    "config = sys.argv\n",
    "if config[0] == \"DCADE_Pattern_Alignment.py\":\n",
    "    print(\"py file exec\")\n",
    "    # DCADE_Pattern_Alignment.py name {\"-c\"(candidate)} {\"-nd\"(no drop)} {\"-t\"(Segment by TopRepeat)}\n",
    "    find_encode = 1\n",
    "    current_path = os.path.join(\".\", \"websites\", config[1])\n",
    "    site_name = config[1]\n",
    "    if \"-c\" in config: brute = 0\n",
    "    if \"-nd\" in config: drop_last = 0\n",
    "    if \"-mt\" in config: seg_method = 0\n",
    "    if \"-train\" in config: train = 1\n",
    "    if \"-txt\" in config: txt = 1\n",
    "\n",
    "#Variable: find_encode\n",
    "#- 0: Use specified encode\n",
    "#- 1: Choose from candidate\n",
    "#--------------------------\n",
    "#Variable: brute (Loop all combination)\n",
    "#- 0: Don't loop\n",
    "#- 1: loop\n",
    "#--------------------------\n",
    "#Variable: drop_last (Last pattern handling)\n",
    "#- 0: handle it\n",
    "#- 1: Only do once MSA\n",
    "#--------------------------\n",
    "#Variable: seg_method (Segmentation method)\n",
    "#- 0: Unique MT\n",
    "#- 1: Split by top repeat\n",
    "#--------------------------\n",
    "#Variable: IGNORE_LEN (loop ignore_len from 0~IGNORE_LEN)\n",
    "#--------------------------\n",
    "#Variable: MINIMAL_REPEAT (Minimal repeat count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_encode():\n",
    "    d = GBM.train_data_prepare()\n",
    "    is_good = d['label'] == 1\n",
    "    return d[is_good]['encode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(string, length):\n",
    "    while len(string) != length:\n",
    "        string = '0' + string\n",
    "    return string\n",
    "\n",
    "sys.setrecursionlimit(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(current_path):\n",
    "    # Input File\n",
    "\n",
    "    input_file_path = os.path.join(current_path, \"TableA.txt\")\n",
    "    print(input_file_path)\n",
    "\n",
    "    # Read File\n",
    "\n",
    "    name, f = read_file(input_file_path)\n",
    "    return name, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mc(col):\n",
    "    # Check MC\n",
    "    MC_CHECK = True\n",
    "    if 'MC' in col:\n",
    "        MC_CHECK = False\n",
    "        print(\"Warning!, MC found!\")\n",
    "    else: print(\"Safe\")\n",
    "    return MC_CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_mt(col, tecid):\n",
    "    # Find Unique MT\n",
    "\n",
    "    tec_dict = {}\n",
    "    unique_mt = []\n",
    "    for node in range(len(col)):\n",
    "        if col[node] == 'MT':\n",
    "            if tecid[node] not in tec_dict.keys():\n",
    "                tec_dict[tecid[node]] = [node]\n",
    "            else:\n",
    "                tec_dict[tecid[node]].append(node)\n",
    "    for key in tec_dict.keys():\n",
    "        if len(tec_dict[key]) == 1:\n",
    "            unique_mt += tec_dict[key]\n",
    "    print(\"Unique MT's index:\\n\", unique_mt)\n",
    "    return unique_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_segment(lock, encode_col, encode_option, unique_mt, ignore_len, MINIMAL_REPEAT):  \n",
    "    node_encode = node_op.encode_node(encode_col, encode_option, len(encode_col[0]))\n",
    "    whole_string, node_dict, index_dict = node_encode\n",
    "    # node_dict:  code -> node num\n",
    "    # index_dict: code -> first index\n",
    "\n",
    "    inv_node_dict = {v: k for k, v in node_dict.items()} #    node num -> code\n",
    "    inv_index_dict = {v: k for k, v in index_dict.items()} # index num -> code\n",
    "    if seg_method == 0:\n",
    "        segments = node_op.segment_mt(unique_mt, whole_string)\n",
    "        record_seg = node_op.mt_record_seg(lock, segments, ignore_len, index_dict, inv_node_dict, MINIMAL_REPEAT)\n",
    "    if seg_method == 1:\n",
    "        segments, record_seg = node_op.segment_top(lock, whole_string, ignore_len, index_dict, inv_node_dict, MINIMAL_REPEAT)\n",
    "\n",
    "    all_seqs = node_op.get_all_seq(record_seg, segments)\n",
    "    return whole_string, segments, record_seg, all_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate():\n",
    "    if brute == 1:\n",
    "        candidate = []\n",
    "        if model_predict == 0:\n",
    "            for i in range(8, 512):\n",
    "                tmp = binary('{0:b}'.format(i), 9)\n",
    "                if len(node_op.find_all_indexes(tmp, '1')) > 3:\n",
    "                    candidate.append(tmp)\n",
    "        else:\n",
    "            if train == 1:\n",
    "                for i in range(1, 512):\n",
    "                    candidate.append(binary('{0:b}'.format(i), 9))\n",
    "            else:\n",
    "                for i in get_good_encode():\n",
    "                    candidate.append(binary(str(i), 9))\n",
    "    else:\n",
    "        with open('./good_encode.txt', 'rb') as f:\n",
    "            candidate = pickle.load(f)\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node(text):\n",
    "    complex_text = text.split(\" :: \")\n",
    "    clean_text = \"\"\n",
    "    clean_node = \"\"\n",
    "    for s in range(len(complex_text)):\n",
    "        if s % 2 == 0:\n",
    "            clean_text += complex_text[s]\n",
    "            if s != len(complex_text)-3 and len(complex_text) > 3:\n",
    "                clean_text += \" \"\n",
    "        elif s != len(complex_text)-1:\n",
    "            clean_node += complex_text[s]\n",
    "            if s != len(complex_text)-2 and len(complex_text) > 3:\n",
    "                clean_node += \" \"\n",
    "    return clean_text, clean_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job(lock, jobs, done, encode_col, unique_mt, best, MINIMAL_REPEAT, model_predict):\n",
    "    try:\n",
    "        while True:\n",
    "            j = jobs.get()\n",
    "            if j is None:\n",
    "                break\n",
    "            option, ignore_len = j\n",
    "\n",
    "            check_dict = {}\n",
    "            result = []\n",
    "            result.append(str(option))\n",
    "            result.append(ignore_len)\n",
    "\n",
    "            encode_option = option\n",
    "            whole_string, segments, record_seg, all_seqs = encode_and_segment(lock, encode_col, encode_option, unique_mt, ignore_len, MINIMAL_REPEAT)\n",
    "\n",
    "            if len(record_seg) > 0:\n",
    "                check_dict[str(all_seqs)] = 1\n",
    "                seq_score = []\n",
    "                delta_list = []\n",
    "                data_len_list = []\n",
    "                density_list = []\n",
    "                overlap_list = []\n",
    "                variance_list = []\n",
    "                total_repeat = 0\n",
    "                total_delta = 0\n",
    "                label_check = 0\n",
    "                top_seg = (0, 0) # Indicate (rep_time, seg_id)\n",
    "                for seg_idx in range(len(all_seqs)):\n",
    "                    appear = {}\n",
    "                    score = 0.0\n",
    "                    length_min_max = [999, 0]\n",
    "                    repeat_time = len(all_seqs[seg_idx])\n",
    "                    if repeat_time > top_seg[0]:\n",
    "                        top_seg = (repeat_time, seg_idx)\n",
    "                    data_count = 0\n",
    "                    total_len = 0\n",
    "                    overlap = {}\n",
    "                    for pattern in all_seqs[seg_idx][:-1]:\n",
    "                        if pattern not in overlap.keys():\n",
    "                            overlap[pattern] = 0\n",
    "                        else: overlap[pattern] += 1\n",
    "                        data_count += 1\n",
    "                        total_len += len(pattern)\n",
    "                        if len(pattern) < length_min_max[0]:\n",
    "                            length_min_max[0] = len(pattern)\n",
    "                        if len(pattern) > length_min_max[1]:\n",
    "                            length_min_max[1] = len(pattern)\n",
    "\n",
    "                    if data_count > 0: mean = total_len/data_count\n",
    "                    else: mean = 0\n",
    "                    \n",
    "                    variance_sum = 0\n",
    "                    for p in list(overlap.keys()):\n",
    "                        for times in range(overlap[p]):\n",
    "                            variance_sum += pow(len(p) - mean, 2)\n",
    "                    \n",
    "                    total_repeat += repeat_time\n",
    "                    \n",
    "                    delta_list.append(length_min_max[1] - length_min_max[0])\n",
    "                    if data_count > 0:\n",
    "                        data_len_list.append(total_len/data_count)\n",
    "                        variance_list.append(variance_sum/data_count)\n",
    "                    else:\n",
    "                        data_len_list.append(0)\n",
    "                        variance_list.append(999)\n",
    "                    if total_len > 0:\n",
    "                        density_list.append((data_count*len(record_seg[seg_idx][1][1]))/total_len)\n",
    "                    else:\n",
    "                        density_list.append(0)\n",
    "                    overlap_list.append(overlap)\n",
    "                    a = cosine_similarity(node_op.to_vector(all_seqs)[seg_idx])\n",
    "                    score = min([min(s) for s in a])\n",
    "\n",
    "                    # Heuristic Limitation\n",
    "                    if model_predict == 0:\n",
    "                        if length_min_max[1] == 1 or length_min_max[0] == 1:\n",
    "                            seq_score.append(score * 0.1)\n",
    "                        elif length_min_max[1] > 12: seq_score.append(0)\n",
    "                        elif length_min_max[1] == length_min_max[0]:\n",
    "                            seq_score.append(score)\n",
    "                        else:\n",
    "                            delta = delta_list[seg_idx]\n",
    "                            if delta <= 2: seq_score.append(score)\n",
    "                            elif delta <= 3: seq_score.append(score*0.7)\n",
    "                            elif delta <= 4: seq_score.append(score*0.6)\n",
    "                            elif delta <= 5: seq_score.append(score*0.5)\n",
    "                            elif delta <= 6: seq_score.append(score*0.4)\n",
    "                            else: seq_score.append(score*0.3)\n",
    "                    else: seq_score.append(score)\n",
    "\n",
    "                total_score = 0\n",
    "                for s in range(len(record_seg)):\n",
    "                    total_score += seq_score[s]\n",
    "                if 0.0 in seq_score: total_score = 0\n",
    "                average = total_score/len(record_seg)\n",
    "\n",
    "                result.append(top_seg[0])\n",
    "                result.append(delta_list[top_seg[1]])\n",
    "                result.append(seq_score[top_seg[1]])\n",
    "                result.append(data_len_list[top_seg[1]])\n",
    "                result.append(density_list[top_seg[1]])\n",
    "                result.append(variance_list[top_seg[1]])\n",
    "                result.append(max(overlap_list[top_seg[1]].values()))\n",
    "\n",
    "                #This line for output train label\n",
    "                #---------------------\n",
    "                #if top_seg[0] == 12:\n",
    "                #    label_check = 1\n",
    "                #---------------------\n",
    "\n",
    "                if label_check == 1: result.append(1)\n",
    "                else: result.append(0)\n",
    "                if average >= best['score'] and not model_predict:\n",
    "                    print(\"\\nBest Update\\n\")\n",
    "                    best['score'] = average\n",
    "                    best['Set count'] = len(record_seg)\n",
    "                    best['option'] = option\n",
    "                    best['ignore_len'] = ignore_len\n",
    "            else:\n",
    "                for _ in range(8):\n",
    "                    result.append(0)\n",
    "            jobs.task_done()\n",
    "            done.put(result)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_brute(lock, jobs, done, encode_col, unique_mt, MINIMAL_REPEAT, model_predict):\n",
    "    # Generate features for each encoding\n",
    "\n",
    "    best = {'option':'000000000', 'score': 0, 'ignore_len': 0}\n",
    "    if find_encode == 1:\n",
    "        # Parameter\n",
    "        \n",
    "        encode = []           # The encoding\n",
    "        ign_len = []          # Minimal top repeat length\n",
    "        label = []            # Label\n",
    "        \n",
    "        # Features\n",
    "        \n",
    "        rep_time = []         # Number of repeat for this encoding\n",
    "        data_block_delta = [] # Length of gap between the longest and shortest record of the top repeat\n",
    "        similarity = []       # Score for the top repeat\n",
    "        data_block_len = []   # The length of data block\n",
    "        top_rep_density = []  # The density of the top repeat\n",
    "        top_rep_variance = [] # The variance of the top repeat\n",
    "        top_rep_overlap = []  # Number of overlapped repeats\n",
    "        \n",
    "        progress = 0\n",
    "        progress_line = [0, 25, 50, 75, 95, 100, 101]\n",
    "        line_count = 0\n",
    "\n",
    "        candidate = get_candidate()\n",
    "\n",
    "        total_progress = len(candidate * (IGNORE_LEN+1))\n",
    "        \n",
    "        num_cpus = int(multiprocessing.cpu_count())\n",
    "        processes = []\n",
    "        \n",
    "        print(\"CPU { \", end='')\n",
    "        for cpu in range(num_cpus):\n",
    "            p = multiprocessing.Process(target=process_job, args=(lock, jobs, done, encode_col, unique_mt, best, MINIMAL_REPEAT, model_predict))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            print(cpu, end=' ')\n",
    "        print(\"} Start\")\n",
    "        \n",
    "        job_count = 0\n",
    "        for ignore_len in range(IGNORE_LEN+1):\n",
    "            for option in candidate:\n",
    "                jobs.put((option, ignore_len))\n",
    "                job_count += 1\n",
    "        \n",
    "        for i in range(job_count):\n",
    "            result = done.get()\n",
    "            encode.append(result[0])\n",
    "            ign_len.append(result[1])\n",
    "            rep_time.append(result[2])\n",
    "            data_block_delta.append(result[3])\n",
    "            similarity.append(result[4])\n",
    "            data_block_len.append(result[5])\n",
    "            top_rep_density.append(result[6])\n",
    "            top_rep_variance.append(result[7])\n",
    "            top_rep_overlap.append(result[8])\n",
    "            label.append(result[9])\n",
    "        \n",
    "        jobs.join()\n",
    "        print(\"Job All CLear\")\n",
    "        \n",
    "        for _ in range(num_cpus):\n",
    "            jobs.put(None)\n",
    "        \n",
    "        for worker in processes:\n",
    "            worker.terminate()\n",
    "            worker.join()\n",
    "        \n",
    "        data = pd.DataFrame(\n",
    "            np.transpose(\n",
    "                np.array(\n",
    "                    [encode, ign_len, rep_time, data_block_delta, similarity, data_block_len, top_rep_density, top_rep_variance, top_rep_overlap, label]\n",
    "                )\n",
    "            ), columns=[\"encode\", \"ign_len\", \"rep_time\", \"data_block_delta\", \"similarity\", \"data_block_len\", \"top_rep_density\", \"top_rep_variance\", \"top_rep_overlap\", \"label\"]\n",
    "        )\n",
    "    if model_predict == 0:\n",
    "        print(\"Best:\", best)\n",
    "        return best\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_name, [content, recb_start, recb_end, tag, ids, classes, pathid, parentid, tecid, cecid, encoding, col, others] = read_table(current_path)\n",
    "    if not check_mc(col): return 1\n",
    "    unique_mt = find_unique_mt(col, tecid)\n",
    "    encode_col = [tag, ids, classes, pathid, parentid, tecid, cecid, encoding, col]\n",
    "    \n",
    "    lock = multiprocessing.Lock()\n",
    "    jobs = multiprocessing.JoinableQueue()\n",
    "    done = multiprocessing.Queue()\n",
    "    \n",
    "    if model_predict:\n",
    "        data = auto_brute(lock, jobs, done, encode_col, unique_mt, MINIMAL_REPEAT, model_predict)\n",
    "        # Save each encoding to test file\n",
    "        data.to_csv(\"./GBM/test.csv\")\n",
    "        # Predict test file by pre-trained model\n",
    "        \n",
    "        predict_encode, predict_ign_len = GBM.GBM_predict(\"test\", \"model\")\n",
    "        if train == 1:\n",
    "            data.to_csv(\"./GBM/need_label_\" + site_name + \".csv\")\n",
    "            return 3\n",
    "        if predict_encode == 0: return 2\n",
    "        predict_encode = binary(str(predict_encode), 9)\n",
    "    else: best = auto_brute(lock, jobs, done, encode_col, unique_mt, MINIMAL_REPEAT, model_predict)\n",
    "    \n",
    "    # Using prediction encoding to build suffix tree and segment data block\n",
    "\n",
    "    if find_encode == 1:\n",
    "        if model_predict == 1:\n",
    "            encode_option = predict_encode\n",
    "            ignore_len = predict_ign_len\n",
    "        else:\n",
    "            encode_option = best['option']\n",
    "            ignore_len = best['ignore_len']\n",
    "    else:\n",
    "        encode_option = '000101001'\n",
    "        ignore_len = 3\n",
    "\n",
    "    whole_string, segments, record_seg, all_seqs = encode_and_segment(lock, encode_col, encode_option, unique_mt, ignore_len, MINIMAL_REPEAT)\n",
    "    \n",
    "    # MSA\n",
    "\n",
    "    import cstar\n",
    "    removed_whole_string = whole_string\n",
    "    for seg_idx in range(len(all_seqs)):\n",
    "        json_result = []\n",
    "        #print(\"First round MSA\\n\", \"=\"*100)\n",
    "        scores = [5, -4, -3] # matchScore, mismatchScore, gapScore\n",
    "        if len(all_seqs[seg_idx][:-1]) == 1:\n",
    "            msa = all_seqs[seg_idx][:-1]\n",
    "        else:\n",
    "            msa = cstar.CenterStar(scores, all_seqs[seg_idx][:-1]).msa()\n",
    "        trans_dict = {}\n",
    "        last_c = '-'\n",
    "        end_idx = 0\n",
    "\n",
    "        for i in range(len(msa)):\n",
    "            if msa[i][-1] != '-' and last_c not in trans_dict.keys() and drop_last == 0:\n",
    "                last_c = msa[i][-1]\n",
    "                trans_dict[last_c] = msa[i].replace('-', '')\n",
    "            if msa[i].replace('-', '') not in trans_dict.keys():\n",
    "                trans_dict[msa[i].replace('-', '')] = msa[i]\n",
    "            else: pass\n",
    "        #for i in all_seqs[seg_idx][:-1]: print(i, \"\\n\\t\\t-> \", trans_dict[i])\n",
    "        if drop_last == 0:\n",
    "            #print('='*100, \"\\nSecond round MSA\\n\")\n",
    "            msa_2 = cstar.CenterStar(scores, msa+[all_seqs[seg_idx][-1]]).msa()\n",
    "            trans_dict_2 = {}\n",
    "\n",
    "            for i in range(len(msa_2)):\n",
    "                if msa_2[i].replace('-', '') not in trans_dict_2.keys():\n",
    "                    trans_dict_2[msa_2[i].replace('-', '')] = msa_2[i]\n",
    "                else: pass\n",
    "            for idx in range(len(trans_dict_2[trans_dict[last_c]])):\n",
    "                if trans_dict_2[trans_dict[last_c]][idx] == last_c:\n",
    "                    end_idx = idx\n",
    "\n",
    "            json_schema = [{} for i in range(len(trans_dict_2[list(trans_dict_2.keys())[0]]))]\n",
    "            schema_check = [0 for i in range(len(trans_dict_2[list(trans_dict_2.keys())[0]]))]\n",
    "            set_type = [\"\" for i in range(len(trans_dict_2[list(trans_dict_2.keys())[0]]))]\n",
    "        else:\n",
    "            json_schema = [{} for i in range(len(trans_dict[list(trans_dict.keys())[0]]))]\n",
    "            schema_check = [0 for i in range(len(trans_dict[list(trans_dict.keys())[0]]))]\n",
    "            set_type = [\"\" for i in range(len(trans_dict[list(trans_dict.keys())[0]]))]\n",
    "\n",
    "        # Set txt & json\n",
    "        with open(os.path.join(current_path, \"Set-\" + str(seg_idx+1) + \".txt\"), 'w', encoding='utf-8') as file:\n",
    "            file.write(\"ColType\\t\")\n",
    "            write_tmp = []\n",
    "            for page in range(len(others)):\n",
    "                #json_page = []\n",
    "                output_dict = {} # Record which pattern is used\n",
    "                if drop_last == 1:\n",
    "                    length = len(all_seqs[seg_idx]) - 1\n",
    "                else:\n",
    "                    length = len(all_seqs[seg_idx])\n",
    "                for s in range(length):\n",
    "                    json_set = []\n",
    "                    write_tmp.append(str(seg_idx+1) + \"-\" + str(page) + \"-\" + str(s+1) + \"\\t\")\n",
    "                    json_set.append(str(seg_idx+1) + \"-\" + str(page) + \"-\" + str(s+1))\n",
    "                    tmp = node_op.find_all_indexes(whole_string, record_seg[seg_idx][1][1])\n",
    "                    if record_seg[seg_idx][1][1] not in output_dict.keys():\n",
    "                        output_dict[record_seg[seg_idx][1][1]] = 0\n",
    "                    else:\n",
    "                        output_dict[record_seg[seg_idx][1][1]] += 1\n",
    "                    #print(\"start:\", tmp[output_dict[seqs[s]]], others[page][tmp[output_dict[seqs[s]]]])\n",
    "                    idx = 0\n",
    "                    if drop_last == 0:\n",
    "                        for c in range(len(trans_dict_2[all_seqs[seg_idx][s]])):\n",
    "                            if trans_dict_2[all_seqs[seg_idx][s]][c] == '-':\n",
    "                                write_tmp.append('\\t')\n",
    "                                json_set.append('')\n",
    "                            else:\n",
    "                                clean_text, clean_node = split_node(others[page][tmp[output_dict[record_seg[seg_idx][1][1]]]+idx])\n",
    "                                write_tmp.append(clean_node + \"\\t\")\n",
    "                                json_set.append(clean_text)\n",
    "                                removed_whole_string = list(removed_whole_string)\n",
    "                                removed_whole_string[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx] = \"-\"\n",
    "                                removed_whole_string = \"\".join(removed_whole_string)\n",
    "                                if schema_check[c] == 0:\n",
    "                                    schema_check[c] = 1\n",
    "                                    json_schema[c][\"PathId\"] = pathid[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    json_schema[c][\"ParentId\"] = parentid[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx].split(':')[0].replace('\\\"', '')\n",
    "                                    json_schema[c][\"Encoding\"] = encoding[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    json_schema[c][\"CECId\"] = cecid[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    json_schema[c][\"TECId\"] = tecid[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    json_schema[c][\"ColType\"] = col[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    set_type[c] = col[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                idx += 1\n",
    "                        if len(list(c for c in write_tmp if c != '\\t' and c != '')) != 1:\n",
    "                            write_tmp.append('\\n')\n",
    "                    else:\n",
    "                        for c in range(len(trans_dict[all_seqs[seg_idx][s]])):\n",
    "                            if trans_dict[all_seqs[seg_idx][s]][c] == '-':\n",
    "                                write_tmp.append('\\t')\n",
    "                                json_set.append('')\n",
    "                            else:\n",
    "                                clean_text, clean_node = split_node(others[page][tmp[output_dict[record_seg[seg_idx][1][1]]]+idx])\n",
    "                                if txt:\n",
    "                                    write_tmp.append(clean_text + \"\\t\")\n",
    "                                else:\n",
    "                                    write_tmp.append(clean_node + \"\\t\")\n",
    "                                json_set.append(clean_text)\n",
    "                                removed_whole_string = list(removed_whole_string)\n",
    "                                removed_whole_string[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx] = \"-\"\n",
    "                                removed_whole_string = \"\".join(removed_whole_string)\n",
    "                                if schema_check[c] == 0:\n",
    "                                    schema_check[c] = 1\n",
    "                                    json_schema[c][\"PathId\"] = pathid[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    json_schema[c][\"ParentId\"] = parentid[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx].split(':')[0].replace('\\\"', '')\n",
    "                                    json_schema[c][\"Encoding\"] = encoding[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    json_schema[c][\"CECId\"] = cecid[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    json_schema[c][\"TECId\"] = tecid[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    json_schema[c][\"ColType\"] = col[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                    set_type[c] = col[tmp[output_dict[record_seg[seg_idx][1][1]]]+idx]\n",
    "                                idx += 1\n",
    "                        if len(list(c for c in write_tmp if c != '\\t' and c != '')) != 1:\n",
    "                            write_tmp.append('\\n')\n",
    "                    #json_page.append(json_set)\n",
    "                    json_result.append(json_set)\n",
    "                #json_result.append(json_page)\n",
    "            for c in set_type:\n",
    "                file.write(c + \"\\t\")\n",
    "            file.write(\"\\n\")\n",
    "            for w in write_tmp:\n",
    "                file.write(w)\n",
    "        with open(os.path.join(current_path, \"Set-\" + str(seg_idx+1) + \".json\"), 'w') as json_file:\n",
    "            json.dump(json_result, json_file)\n",
    "        with open(os.path.join(current_path, \"SchemaSet-\" + str(seg_idx+1) + \".json\"), 'w') as json_file:\n",
    "            json.dump(json_schema, json_file)\n",
    "    \n",
    "    # Modified TableA txt & json Output\n",
    "\n",
    "    json_table = []\n",
    "    json_schema = []\n",
    "    check = False\n",
    "    txt_table = \"ColType\\t\"\n",
    "    set_count = 0\n",
    "    for node in range(len(removed_whole_string)):\n",
    "        schema_dict = {}\n",
    "        if removed_whole_string[node] == '-':\n",
    "            if check == False:\n",
    "                check = True\n",
    "                set_count += 1\n",
    "                txt_table += 'Set-' + str(set_count)\n",
    "                txt_table += \"\\t\"\n",
    "                schema_dict[\"Encoding\"] = -1\n",
    "                schema_dict[\"TECId\"] = \"1\"\n",
    "                schema_dict[\"ColType\"] = \"Set-\" + str(set_count)\n",
    "                json_schema.append(schema_dict)\n",
    "            else: pass\n",
    "        else:\n",
    "            if check == True:\n",
    "                check = False\n",
    "            txt_table += col[node]\n",
    "            txt_table += \"\\t\"\n",
    "            schema_dict[\"PathId\"] = pathid[node]\n",
    "            schema_dict[\"ParentId\"] = parentid[node].split(':')[0].replace('\\\"', '')\n",
    "            if encoding[node] != ' ':\n",
    "                schema_dict[\"Encoding\"] = int(encoding[node])\n",
    "            else:\n",
    "                schema_dict[\"Encoding\"] = ''\n",
    "            schema_dict[\"CECId\"] = cecid[node]\n",
    "            schema_dict[\"TECId\"] = tecid[node]\n",
    "            schema_dict[\"ColType\"] = col[node]\n",
    "            json_schema.append(schema_dict)\n",
    "    txt_table += \"\\n\"\n",
    "            \n",
    "    for page in range(len(others)):\n",
    "        txt_table += file_name[page] + \"\\t\"\n",
    "        json_page = []\n",
    "        set_count = 0\n",
    "        check = False\n",
    "        for node in range(len(removed_whole_string)):\n",
    "            if removed_whole_string[node] == '-':\n",
    "                if check == False:\n",
    "                    check = True\n",
    "                    set_count += 1\n",
    "                    json_page.append(str(set_count) + '-' + str(page))\n",
    "                    txt_table += str(set_count) + '-' + str(page)\n",
    "                    txt_table += \"\\t\"\n",
    "                else: pass\n",
    "            else:\n",
    "                if check == True:\n",
    "                    check = False\n",
    "                clean_text, clean_node = split_node(others[page][node])\n",
    "                json_page.append(clean_text)\n",
    "                if txt:\n",
    "                    txt_table += clean_text\n",
    "                else:\n",
    "                    txt_table += clean_node\n",
    "                txt_table += \"\\t\"\n",
    "        json_table.append(json_page)\n",
    "        txt_table += \"\\n\"\n",
    "    with open(os.path.join(current_path, \"TableA.json\"), 'w') as json_file:\n",
    "        json.dump(json_table, json_file)\n",
    "    with open(os.path.join(current_path, \"SchemaTableA.json\"), 'w') as json_file:\n",
    "        json.dump(json_schema, json_file)\n",
    "    with open(os.path.join(current_path, \"TableA_NC.txt\"), \"w\", encoding='utf-8') as table_file:\n",
    "        table_file.write(txt_table)\n",
    "    \n",
    "    # Save good encoding for next use\n",
    "\n",
    "    with open('./good_encode.txt', 'rb') as f:\n",
    "        candidate = pickle.load(f)\n",
    "    if encode_option not in candidate and brute == 1:\n",
    "        with open('./good_encode.txt', 'wb') as f:\n",
    "            candidate.append(encode_option)\n",
    "            print(\"Append:\", encode_option)\n",
    "            pickle.dump(candidate, f)\n",
    "\n",
    "    #print(candidate)\n",
    "\n",
    "    if brute == 1:\n",
    "        with open('./good_encode.txt', 'rb') as f:\n",
    "            candidate = pickle.load(f)\n",
    "\n",
    "    #candidate = []\n",
    "\n",
    "    if brute == 1:\n",
    "        with open('./good_encode.txt', 'wb') as f:\n",
    "            pickle.dump(candidate, f)\n",
    "\n",
    "    # Show model selected encoding\n",
    "\n",
    "    if model_predict == 1:\n",
    "        print(len(all_seqs), \"Set\\nModel Select: \", predict_encode, predict_ign_len)\n",
    "    else:\n",
    "        print(len(all_seqs), \"Set\\nBEST: \", best)\n",
    "    print(record_seg)\n",
    "\n",
    "    #cols = [\"tag\", \"ids\", \"classes\", \"pathid\", \"parentid\", \"tecid\", \"cecid\", \"encoding\", \"col\"]\n",
    "    #for c in range(len(best['option'])):\n",
    "    #    if best['option'][c] == '1':\n",
    "    #        print(cols[c])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    s = main()\n",
    "    if s == 1: print(\"MC Occur, PASS\")\n",
    "    elif s == 2:\n",
    "        os.system(\"mv ./GBM/test.csv ./GBM/need_label_\" + site_name + \".csv\")\n",
    "        print(\"Model no suggest, rename test file to need_label.csv\")\n",
    "    elif s == 3: print(\"Train file created, name: need_label_\" + site_name + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
