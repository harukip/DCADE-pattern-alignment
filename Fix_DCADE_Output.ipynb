{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from suffix_trees import STree\n",
    "import suffix_tree\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STreeEx():\n",
    "    def __init__(self, sTree):\n",
    "        # 傳入原套件後綴樹\n",
    "        self.sTree = sTree\n",
    "\n",
    "    def lrs(self):\n",
    "        # 最深非葉節點\n",
    "        deepestNode = self._find_lrs(self.sTree.root)\n",
    "        start = deepestNode.idx\n",
    "        end = deepestNode.idx + deepestNode.depth\n",
    "        return self.sTree.word[start:end]\n",
    "\n",
    "    def _find_lrs(self, node):\n",
    "        nodes = [self._find_lrs(n)\n",
    "            for (n,_) in node.transition_links\n",
    "            # 排除葉節點\n",
    "            if n.transition_links != []]\n",
    "\n",
    "        if nodes == []:\n",
    "            return node\n",
    "        \n",
    "        deepestNode = max(nodes, key=lambda n: n.depth)\n",
    "        return deepestNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"./input/TableA.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = []\n",
    "recb_start = 0\n",
    "recb_end = 0\n",
    "tag = []\n",
    "ids = []\n",
    "classes = []\n",
    "pathid = []\n",
    "parentid = []\n",
    "tecid = []\n",
    "cecid = []\n",
    "encoding = []\n",
    "col = []\n",
    "others = []\n",
    "with open(input_file_path) as file:\n",
    "    line = file.readline()\n",
    "    while(line):\n",
    "        line = file.readline()\n",
    "        tmp = line.split('\\t')\n",
    "        if tmp[0] == \"Content\":\n",
    "            content = tmp[1:]\n",
    "        elif tmp[0] == \"RecB\":\n",
    "            tmp = tmp[1:]\n",
    "            for index in range(len(tmp)):\n",
    "                if tmp[index] == \"SDR\":\n",
    "                    recb_start = index\n",
    "                elif tmp[index] == \"EDR\":\n",
    "                    recb_end = index\n",
    "        elif tmp[0] == \"Tag\":\n",
    "            tag = tmp[1:]\n",
    "        elif tmp[0] == \"ID\":\n",
    "            ids = tmp[1:]\n",
    "        elif tmp[0] == \"Class\":\n",
    "            classes = tmp[1:]\n",
    "        elif tmp[0] == \"PathId\":\n",
    "            pathid = tmp[1:]\n",
    "        elif tmp[0] == \"ParentId\":\n",
    "            parentid = tmp[1:]\n",
    "        elif tmp[0] == \"TECId\":\n",
    "            tecid = tmp[1:]\n",
    "        elif tmp[0] == \"CECId\":\n",
    "            cecid = tmp[1:]\n",
    "        elif tmp[0] == \"Encoding\":\n",
    "            encoding = tmp[1:]\n",
    "        elif tmp[0] == \"Col Type\":\n",
    "            col = tmp[1:]\n",
    "        else:\n",
    "            others.append(tmp[1:])\n",
    "others = others[0:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique MT's index:\n",
      " [3, 4, 5, 6, 8, 10, 16, 18, 19, 20, 22, 23, 25, 28, 81, 136, 279, 281, 284, 291, 303, 306, 308]\n"
     ]
    }
   ],
   "source": [
    "tec_dict = {}\n",
    "unique_mt = []\n",
    "for node in range(len(col)):\n",
    "    if col[node] == 'MT':\n",
    "        if tecid[node] not in tec_dict.keys():\n",
    "            tec_dict[tecid[node]] = [node]\n",
    "        else:\n",
    "            tec_dict[tecid[node]].append(node)\n",
    "for key in tec_dict.keys():\n",
    "    if len(tec_dict[key]) == 1:\n",
    "        unique_mt += tec_dict[key]\n",
    "print(\"Unique MT's index:\\n\", unique_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate node in to unicode for suffix tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `encode` to use different attribute to encode leafnodes.\n",
    "- 0: **Pathid** + **Parentid** + **TECid** + **CECid** + **Encoding** + **Column Type**\n",
    "- 1: **Id** + **Class** + **Pathid** + **CECid**\n",
    "- 2: **Pathid** + **Parentid** + **CECid**\n",
    "<br><br>\n",
    "Leafnodes will have same encoding if they have same attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      "0,\"1:[1, 2]\", \n",
      "\n",
      "Convert to Unicode String:\n",
      " \u0000\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t\n",
      "\u000b",
      "\f",
      "\r",
      "\f",
      "\u000e\u000f\u0010\u0011\u0012\u0013\u0014\u0015\u0016\u0017\u0018\u0019\u001a\u001b\u001c",
      "\u001d",
      "\u001e",
      "\u001d",
      "\u001f\u001d",
      "!\u001d",
      "\"\u001d",
      "#\u001d",
      "$\u001d",
      "%\u001d",
      "&\u001d",
      "'\u001d",
      "(\u001d",
      ")\u001d",
      "*\u001d",
      "\u001c",
      "\u001d",
      "\u001e",
      "\u001d",
      "\u001f\u001d",
      "!\u001d",
      "\"\u001d",
      "#\u001d",
      "$\u001d",
      "%\u001d",
      "&\u001d",
      "'\u001d",
      "(\u001d",
      ")\u001d",
      "*\u001d",
      "+,./.0.1.2.3.4.5.6.7.8.9.:.;.<.=.>.?.@.A.B.C.D.E.F.G.H.I.JJKLLMKNKOPJJKLLQNKOKJJKLLMKNKOKJJKLLMKNKOPJJKLLMQNKOKJJKLLMKNKOKJJKLLMKNKOKJJKLLMKNKOKJJKLLKNKOPJJKLLMKNKOPJJKLLMQNKOKJJKLLKNKOKR\f",
      "S\f",
      "T\f",
      "U\f",
      "V\f",
      "W\f",
      "X\f",
      "YZ[\\]^_`abcdefghijklmnopqrstu\n"
     ]
    }
   ],
   "source": [
    "encode = 2\n",
    "whole_string = \"\"\n",
    "test = \"\"\n",
    "node_dict = {} # code -> node num\n",
    "code_dict = {} # code -> content\n",
    "index_dict = {} # code -> first index\n",
    "index = 0\n",
    "for node in range(len(pathid)):\n",
    "    if encode == 0:\n",
    "        code = pathid[node] + \",\" + parentid[node] + \",\" + tecid[node] + \",\" + cecid[node] + \",\" + encoding[node] + \",\" + col[node]\n",
    "    elif encode == 1:\n",
    "        code = ids[node] + \",\" + classes[node] + \",\" + pathid[node] + \",\" + cecid[node]\n",
    "    elif encode == 2:\n",
    "        code = pathid[node] + \",\" + parentid[node] + \",\" + cecid[node]\n",
    "    #if 'O' in col[node]: continue\n",
    "    if code not in node_dict.keys():\n",
    "        node_dict[code] = index\n",
    "        code_dict[code] = others[0][node]\n",
    "        index_dict[code] = node\n",
    "        whole_string += chr(index)\n",
    "        index += 1\n",
    "        if index == 45 or index == 32:\n",
    "            index+=1\n",
    "    else:\n",
    "        whole_string += chr(node_dict[code])\n",
    "print(\"Example: \")\n",
    "if encode == 0:\n",
    "    print(pathid[0] + \",\" + parentid[0] + \",\" + tecid[0] + \",\" + cecid[0] + \",\" + encoding[0] + \",\" + col[0])\n",
    "elif encode == 1:\n",
    "    print(ids[0] + \",\" + classes[0] + \",\" + pathid[0] + \",\" + cecid[0])\n",
    "if encode == 2:\n",
    "    print(pathid[0] + \",\" + parentid[0] + \",\" + cecid[0])\n",
    "inv_node_dict = {v: k for k, v in node_dict.items()} # node num -> code\n",
    "inv_index_dict = {v: k for k, v in index_dict.items()} # index num -> code\n",
    "print(\"\\nConvert to Unicode String:\\n\", whole_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['\\x1c\\x1d\\x1e\\x1d\\x1f\\x1d!\\x1d\"\\x1d#\\x1d$\\x1d%\\x1d&\\x1d\\'\\x1d(\\x1d)\\x1d*\\x1d\\x1c\\x1d\\x1e\\x1d\\x1f\\x1d!\\x1d\"\\x1d#\\x1d$\\x1d%\\x1d&\\x1d\\'\\x1d(\\x1d)\\x1d*\\x1d', ',./.0.1.2.3.4.5.6.7.8.9.:.;.<.=.>.?.@.A.B.C.D.E.F.G.H.', '.JJKLLMKNKOPJJKLLQNKOKJJKLLMKNKOKJJKLLMKNKOPJJKLLMQNKOKJJKLLMKNKOKJJKLLMKNKOKJJKLLMKNKOKJJKLLKNKOPJJKLLMKNKOPJJKLLMQNKOKJJKLLKNKOKR\\x0cS\\x0cT\\x0cU\\x0cV\\x0cW\\x0c', 'defghijklmn']\n"
     ]
    }
   ],
   "source": [
    "segments = []\n",
    "for seg_point in range(len(unique_mt)):\n",
    "    if seg_point != len(unique_mt)-1:\n",
    "        tmp = whole_string[unique_mt[seg_point]+1:unique_mt[seg_point+1]]\n",
    "        if len(tmp) > 10:\n",
    "            segments.append(tmp)\n",
    "segments = list(filter(None, segments))\n",
    "print(len(segments))\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_indexes(input_str, search_str):\n",
    "    l1 = []\n",
    "    length = len(input_str)\n",
    "    index = 0\n",
    "    while index < length:\n",
    "        i = input_str.find(search_str, index)\n",
    "        if i == -1:\n",
    "            return l1\n",
    "        l1.append(i)\n",
    "        index = i + 1\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longest repeat pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'st = STree.STree(whole_string)\\nst = STreeEx(st)\\nlrs = str(st.lrs())\\nprint(\"Length: \", len(lrs))\\nprint(lrs)\\nfor i in lrs:\\n    print(\"Content: \", code_dict[inv_node_dict[ord(i)]])'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"st = STree.STree(whole_string)\n",
    "st = STreeEx(st)\n",
    "lrs = str(st.lrs())\n",
    "print(\"Length: \", len(lrs))\n",
    "print(lrs)\n",
    "for i in lrs:\n",
    "    print(\"Content: \", code_dict[inv_node_dict[ord(i)]])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show maximal repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore maximal repeats if length equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree = suffix_tree.Tree({\\'A\\': whole_string})\\nprint(\"Total Maximal repeats: \", len(tree.maximal_repeats()), \\'\\n\\')\\ngroup = {}\\ntop_repeats = (0, \\'\\')\\nfor C, path in tree.maximal_repeats():\\n    if len(path) == 1: continue\\n    count = 0\\n    print(\"Length: \", len(path), \"Pattern: {\", path, \"}\")\\n    for id_, path2 in tree.find_all (path):\\n        count += 1\\n    print(\"Count: \", count)\\n    if count > top_repeats[0]:\\n        top_repeats = (count, str(path).replace(\\' \\', \\'\\'))\\n    for i in path:\\n        print(\"Content: \", code_dict[inv_node_dict[ord(i)]])\\n    if index_dict[inv_node_dict[ord(path[0])]] not in group.keys():\\n        group[index_dict[inv_node_dict[ord(path[0])]]] = []\\n        group[index_dict[inv_node_dict[ord(path[0])]]].append(path)\\n    else:\\n        group[index_dict[inv_node_dict[ord(path[0])]]].append(path)\\n    print(\"\\n\", \"=\"*50, \"\\n\")'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tree = suffix_tree.Tree({'A': whole_string})\n",
    "print(\"Total Maximal repeats: \", len(tree.maximal_repeats()), '\\n')\n",
    "group = {}\n",
    "top_repeats = (0, '')\n",
    "for C, path in tree.maximal_repeats():\n",
    "    if len(path) == 1: continue\n",
    "    count = 0\n",
    "    print(\"Length: \", len(path), \"Pattern: {\", path, \"}\")\n",
    "    for id_, path2 in tree.find_all (path):\n",
    "        count += 1\n",
    "    print(\"Count: \", count)\n",
    "    if count > top_repeats[0]:\n",
    "        top_repeats = (count, str(path).replace(' ', ''))\n",
    "    for i in path:\n",
    "        print(\"Content: \", code_dict[inv_node_dict[ord(i)]])\n",
    "    if index_dict[inv_node_dict[ord(path[0])]] not in group.keys():\n",
    "        group[index_dict[inv_node_dict[ord(path[0])]]] = []\n",
    "        group[index_dict[inv_node_dict[ord(path[0])]]].append(path)\n",
    "    else:\n",
    "        group[index_dict[inv_node_dict[ord(path[0])]]].append(path)\n",
    "    print(\"\\n\", \"=\"*50, \"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, '\\x1c\\x1d\\x1e\\x1d\\x1f\\x1d!\\x1d\"\\x1d#\\x1d$\\x1d%\\x1d&\\x1d\\'\\x1d(\\x1d)\\x1d*\\x1d')\n",
      "(12, 'JJKLL')\n"
     ]
    }
   ],
   "source": [
    "record_seg = []\n",
    "trees = [suffix_tree.Tree({'A': seg}) for seg in segments]\n",
    "for tree_idx in range(len(trees)):\n",
    "    tree = trees[tree_idx]\n",
    "    #print(\"Total Maximal repeats: \", len(tree.maximal_repeats()), '\\n')\n",
    "    group = {}\n",
    "    top_repeats = (0, '')\n",
    "    for C, path in tree.maximal_repeats():\n",
    "        if len(path) == 1: continue\n",
    "        count = 0\n",
    "        #print(\"Length: \", len(path), \"Pattern: {\", path, \"}\")\n",
    "        for id_, path2 in tree.find_all (path):\n",
    "            count += 1\n",
    "        #print(\"Count: \", count)\n",
    "        if count > top_repeats[0]:\n",
    "            top_repeats = (count, str(path).replace(' ', ''))\n",
    "        \"\"\"for i in path:\n",
    "            print(\"Content: \", code_dict[inv_node_dict[ord(i)]])\"\"\"\n",
    "        if index_dict[inv_node_dict[ord(path[0])]] not in group.keys():\n",
    "            group[index_dict[inv_node_dict[ord(path[0])]]] = []\n",
    "            group[index_dict[inv_node_dict[ord(path[0])]]].append(path)\n",
    "        else:\n",
    "            group[index_dict[inv_node_dict[ord(path[0])]]].append(path)\n",
    "        #print(\"\\n\", \"=\"*50, \"\\n\")\n",
    "    if top_repeats[0] != 0:\n",
    "        record_seg.append((tree_idx, top_repeats[1]))\n",
    "        print(top_repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole String split by top repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_len = 0\\nseqs = []\\npos = find_all_indexes(whole_string, top_repeats[1])\\nfor idx in range(len(pos)):\\n    if idx+1 != len(pos):\\n        seqs.append(whole_string[pos[idx]:pos[idx+1]])\\n        if len(whole_string[pos[idx]:pos[idx+1]]) > max_len:\\n            max_len = len(whole_string[pos[idx]:pos[idx+1]])\\n    else:\\n        seqs.append(whole_string[pos[idx]:pos[idx]+max_len+4])\\nprint(str(seqs))'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"max_len = 0\n",
    "seqs = []\n",
    "pos = find_all_indexes(whole_string, top_repeats[1])\n",
    "for idx in range(len(pos)):\n",
    "    if idx+1 != len(pos):\n",
    "        seqs.append(whole_string[pos[idx]:pos[idx+1]])\n",
    "        if len(whole_string[pos[idx]:pos[idx+1]]) > max_len:\n",
    "            max_len = len(whole_string[pos[idx]:pos[idx+1]])\n",
    "    else:\n",
    "        seqs.append(whole_string[pos[idx]:pos[idx]+max_len+4])\n",
    "print(str(seqs))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\x1c\\x1d\\x1e\\x1d\\x1f\\x1d!\\x1d\"\\x1d#\\x1d$\\x1d%\\x1d&\\x1d\\'\\x1d(\\x1d)\\x1d*\\x1d', '\\x1c\\x1d\\x1e\\x1d\\x1f\\x1d!\\x1d\"\\x1d#\\x1d$\\x1d%\\x1d&\\x1d\\'\\x1d(\\x1d)\\x1d*\\x1d'], ['JJKLLMKNKOP', 'JJKLLQNKOK', 'JJKLLMKNKOK', 'JJKLLMKNKOP', 'JJKLLMQNKOK', 'JJKLLMKNKOK', 'JJKLLMKNKOK', 'JJKLLMKNKOK', 'JJKLLKNKOP', 'JJKLLMKNKOP', 'JJKLLMQNKOK', 'JJKLLKNKOKR\\x0cS\\x0cT']]\n"
     ]
    }
   ],
   "source": [
    "max_len = []\n",
    "all_seqs = []\n",
    "for seg_idx in range(len(record_seg)):\n",
    "    max_len.append(0)\n",
    "    seqs = []\n",
    "    pos = find_all_indexes(segments[record_seg[seg_idx][0]], record_seg[seg_idx][1])\n",
    "    for idx in range(len(pos)):\n",
    "        if idx+1 != len(pos):\n",
    "            seqs.append(segments[record_seg[seg_idx][0]][pos[idx]:pos[idx+1]])\n",
    "            if len(segments[record_seg[seg_idx][0]][pos[idx]:pos[idx+1]]) > max_len[seg_idx]:\n",
    "                max_len[seg_idx] = len(segments[record_seg[seg_idx][0]][pos[idx]:pos[idx+1]])\n",
    "        else:\n",
    "            seqs.append(segments[record_seg[seg_idx][0]][pos[idx]:pos[idx]+max_len[seg_idx]+4])\n",
    "    all_seqs.append(seqs)\n",
    "print(all_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import cstar\\nprint(\"First round MSA\\n\", \"=\"*100)\\nscores = [1, -1, -3] # matchScore, mismatchScore, gapScore\\nmsa = cstar.CenterStar(scores, seqs[:-1]).msa()\\ntrans_dict = {}\\nlast_c = \\'-\\'\\nend_idx = 0\\n\\nfor i in range(len(msa)):\\n    if msa[i][-1] != \\'-\\' and last_c not in trans_dict.keys():\\n        last_c = msa[i][-1]\\n        trans_dict[last_c] = msa[i].replace(\\'-\\', \\'\\')\\n    if msa[i].replace(\\'-\\', \\'\\') not in trans_dict.keys():\\n        trans_dict[msa[i].replace(\\'-\\', \\'\\')] = msa[i]\\n    else: pass\\nfor i in seqs[:-1]:\\n    print(i, \"\\t-> \", trans_dict[i])\\n\\nprint(\\'=\\'*100, \"\\nSecond round MSA\\n\")\\nmsa_2 = cstar.CenterStar(scores, msa+[seqs[-1]]).msa()\\ntrans_dict_2 = {}\\n\\nfor i in range(len(msa_2)):\\n    if msa_2[i].replace(\\'-\\', \\'\\') not in trans_dict_2.keys():\\n        trans_dict_2[msa_2[i].replace(\\'-\\', \\'\\')] = msa_2[i]\\n    else: pass\\nfor idx in range(len(trans_dict_2[trans_dict[last_c]])):\\n    if trans_dict_2[trans_dict[last_c]][idx] == last_c:\\n        end_idx = idx\\nfor i in seqs:\\n    trans_dict_2[i] = trans_dict_2[i][:end_idx+1]\\n    print(i, \"\\t-> \", trans_dict_2[i])\\n\\nwith open(\\'./Set.txt\\', \\'w\\', encoding=\\'utf-8\\') as file:\\n    for page in range(len(others)):\\n        output_dict = {} # Record which pattern is used\\n        for s in range(len(seqs)):\\n            file.write(str(page) + \"-0\" + \"-\" + str(s) + \"\\t\")\\n            tmp = find_all_indexes(whole_string, seqs[s])\\n            if seqs[s] not in output_dict.keys():\\n                output_dict[seqs[s]] = 0\\n            else:\\n                output_dict[seqs[s]] += 1\\n            #print(\"start:\", tmp[output_dict[seqs[s]]], others[page][tmp[output_dict[seqs[s]]]])\\n            idx = 0\\n            for c in range(len(trans_dict_2[seqs[s]])):\\n                if trans_dict_2[seqs[s]][c] == \\'-\\':\\n                    file.write(\\'\\t\\')\\n                else:\\n                    file.write(others[page][tmp[output_dict[seqs[s]]]+idx] + \"\\t\")\\n                    idx += 1\\n            file.write(\\'\\n\\')'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import cstar\n",
    "print(\"First round MSA\\n\", \"=\"*100)\n",
    "scores = [1, -1, -3] # matchScore, mismatchScore, gapScore\n",
    "msa = cstar.CenterStar(scores, seqs[:-1]).msa()\n",
    "trans_dict = {}\n",
    "last_c = '-'\n",
    "end_idx = 0\n",
    "\n",
    "for i in range(len(msa)):\n",
    "    if msa[i][-1] != '-' and last_c not in trans_dict.keys():\n",
    "        last_c = msa[i][-1]\n",
    "        trans_dict[last_c] = msa[i].replace('-', '')\n",
    "    if msa[i].replace('-', '') not in trans_dict.keys():\n",
    "        trans_dict[msa[i].replace('-', '')] = msa[i]\n",
    "    else: pass\n",
    "for i in seqs[:-1]:\n",
    "    print(i, \"\\t-> \", trans_dict[i])\n",
    "\n",
    "print('='*100, \"\\nSecond round MSA\\n\")\n",
    "msa_2 = cstar.CenterStar(scores, msa+[seqs[-1]]).msa()\n",
    "trans_dict_2 = {}\n",
    "\n",
    "for i in range(len(msa_2)):\n",
    "    if msa_2[i].replace('-', '') not in trans_dict_2.keys():\n",
    "        trans_dict_2[msa_2[i].replace('-', '')] = msa_2[i]\n",
    "    else: pass\n",
    "for idx in range(len(trans_dict_2[trans_dict[last_c]])):\n",
    "    if trans_dict_2[trans_dict[last_c]][idx] == last_c:\n",
    "        end_idx = idx\n",
    "for i in seqs:\n",
    "    trans_dict_2[i] = trans_dict_2[i][:end_idx+1]\n",
    "    print(i, \"\\t-> \", trans_dict_2[i])\n",
    "\n",
    "with open('./Set.txt', 'w', encoding='utf-8') as file:\n",
    "    for page in range(len(others)):\n",
    "        output_dict = {} # Record which pattern is used\n",
    "        for s in range(len(seqs)):\n",
    "            file.write(str(page) + \"-0\" + \"-\" + str(s) + \"\\t\")\n",
    "            tmp = find_all_indexes(whole_string, seqs[s])\n",
    "            if seqs[s] not in output_dict.keys():\n",
    "                output_dict[seqs[s]] = 0\n",
    "            else:\n",
    "                output_dict[seqs[s]] += 1\n",
    "            #print(\"start:\", tmp[output_dict[seqs[s]]], others[page][tmp[output_dict[seqs[s]]]])\n",
    "            idx = 0\n",
    "            for c in range(len(trans_dict_2[seqs[s]])):\n",
    "                if trans_dict_2[seqs[s]][c] == '-':\n",
    "                    file.write('\\t')\n",
    "                else:\n",
    "                    file.write(others[page][tmp[output_dict[seqs[s]]]+idx] + \"\\t\")\n",
    "                    idx += 1\n",
    "            file.write('\\n')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First round MSA\n",
      " ====================================================================================================\n",
      "\u001c",
      "\u001d",
      "\u001e",
      "\u001d",
      "\u001f\u001d",
      "!\u001d",
      "\"\u001d",
      "#\u001d",
      "$\u001d",
      "%\u001d",
      "&\u001d",
      "'\u001d",
      "(\u001d",
      ")\u001d",
      "*\u001d",
      " \t->  \u001c",
      "\u001d",
      "\u001e",
      "\u001d",
      "\u001f\u001d",
      "!\u001d",
      "\"\u001d",
      "#\u001d",
      "$\u001d",
      "%\u001d",
      "&\u001d",
      "'\u001d",
      "(\u001d",
      ")\u001d",
      "*\u001d",
      "\n",
      "==================================================================================================== \n",
      "Second round MSA\n",
      "\n",
      "\u001c",
      "\u001d",
      "\u001e",
      "\u001d",
      "\u001f\u001d",
      "!\u001d",
      "\"\u001d",
      "#\u001d",
      "$\u001d",
      "%\u001d",
      "&\u001d",
      "'\u001d",
      "(\u001d",
      ")\u001d",
      "*\u001d",
      " \t->  \u001c",
      "\u001d",
      "\u001e",
      "\u001d",
      "\u001f\u001d",
      "!\u001d",
      "\"\u001d",
      "#\u001d",
      "$\u001d",
      "%\u001d",
      "&\u001d",
      "'\u001d",
      "(\u001d",
      ")\u001d",
      "*\u001d",
      "\n",
      "\u001c",
      "\u001d",
      "\u001e",
      "\u001d",
      "\u001f\u001d",
      "!\u001d",
      "\"\u001d",
      "#\u001d",
      "$\u001d",
      "%\u001d",
      "&\u001d",
      "'\u001d",
      "(\u001d",
      ")\u001d",
      "*\u001d",
      " \t->  \u001c",
      "\u001d",
      "\u001e",
      "\u001d",
      "\u001f\u001d",
      "!\u001d",
      "\"\u001d",
      "#\u001d",
      "$\u001d",
      "%\u001d",
      "&\u001d",
      "'\u001d",
      "(\u001d",
      ")\u001d",
      "*\u001d",
      "\n",
      "First round MSA\n",
      " ====================================================================================================\n",
      "JJKLLMKNKOP \t->  JJKLLMKNKOP\n",
      "JJKLLQNKOK \t->  JJKLL-QNKOK\n",
      "JJKLLMKNKOK \t->  JJKLLMKNKOK\n",
      "JJKLLMKNKOP \t->  JJKLLMKNKOP\n",
      "JJKLLMQNKOK \t->  JJKLLMQNKOK\n",
      "JJKLLMKNKOK \t->  JJKLLMKNKOK\n",
      "JJKLLMKNKOK \t->  JJKLLMKNKOK\n",
      "JJKLLMKNKOK \t->  JJKLLMKNKOK\n",
      "JJKLLKNKOP \t->  JJKLL-KNKOP\n",
      "JJKLLMKNKOP \t->  JJKLLMKNKOP\n",
      "JJKLLMQNKOK \t->  JJKLLMQNKOK\n",
      "==================================================================================================== \n",
      "Second round MSA\n",
      "\n",
      "JJKLLMKNKOP \t->  JJKLLMKNKOP\n",
      "JJKLLQNKOK \t->  JJKLL-QNKOK\n",
      "JJKLLMKNKOK \t->  JJKLLMKNKOK\n",
      "JJKLLMKNKOP \t->  JJKLLMKNKOP\n",
      "JJKLLMQNKOK \t->  JJKLLMQNKOK\n",
      "JJKLLMKNKOK \t->  JJKLLMKNKOK\n",
      "JJKLLMKNKOK \t->  JJKLLMKNKOK\n",
      "JJKLLMKNKOK \t->  JJKLLMKNKOK\n",
      "JJKLLKNKOP \t->  JJKLL-KNKOP\n",
      "JJKLLMKNKOP \t->  JJKLLMKNKOP\n",
      "JJKLLMQNKOK \t->  JJKLLMQNKOK\n",
      "JJKLLKNKOKR\f",
      "S\f",
      "T \t->  JJKLL-KNKOK\n"
     ]
    }
   ],
   "source": [
    "import cstar\n",
    "for seg_idx in range(len(all_seqs)):\n",
    "    print(\"First round MSA\\n\", \"=\"*100)\n",
    "    scores = [1, -1, -3] # matchScore, mismatchScore, gapScore\n",
    "    if len(all_seqs[seg_idx][:-1]) == 1:\n",
    "        msa = all_seqs[seg_idx][:-1]\n",
    "    else:\n",
    "        msa = cstar.CenterStar(scores, all_seqs[seg_idx][:-1]).msa()\n",
    "    trans_dict = {}\n",
    "    last_c = '-'\n",
    "    end_idx = 0\n",
    "\n",
    "    for i in range(len(msa)):\n",
    "        if msa[i][-1] != '-' and last_c not in trans_dict.keys():\n",
    "            last_c = msa[i][-1]\n",
    "            trans_dict[last_c] = msa[i].replace('-', '')\n",
    "        if msa[i].replace('-', '') not in trans_dict.keys():\n",
    "            trans_dict[msa[i].replace('-', '')] = msa[i]\n",
    "        else: pass\n",
    "    for i in all_seqs[seg_idx][:-1]:\n",
    "        print(i, \"\\t-> \", trans_dict[i])\n",
    "\n",
    "    print('='*100, \"\\nSecond round MSA\\n\")\n",
    "    msa_2 = cstar.CenterStar(scores, msa+[all_seqs[seg_idx][-1]]).msa()\n",
    "    trans_dict_2 = {}\n",
    "\n",
    "    for i in range(len(msa_2)):\n",
    "        if msa_2[i].replace('-', '') not in trans_dict_2.keys():\n",
    "            trans_dict_2[msa_2[i].replace('-', '')] = msa_2[i]\n",
    "        else: pass\n",
    "    for idx in range(len(trans_dict_2[trans_dict[last_c]])):\n",
    "        if trans_dict_2[trans_dict[last_c]][idx] == last_c:\n",
    "            end_idx = idx\n",
    "    for i in all_seqs[seg_idx]:\n",
    "        trans_dict_2[i] = trans_dict_2[i][:end_idx+1]\n",
    "        print(i, \"\\t-> \", trans_dict_2[i])\n",
    "\n",
    "    with open('./Set-' + str(seg_idx) + '.txt', 'w', encoding='utf-8') as file:\n",
    "        for page in range(len(others)):\n",
    "            output_dict = {} # Record which pattern is used\n",
    "            for s in range(len(all_seqs[seg_idx])):\n",
    "                file.write(str(page) + \"-\" + str(seg_idx) + \"-\" + str(s) + \"\\t\")\n",
    "                tmp = find_all_indexes(whole_string, all_seqs[seg_idx][s])\n",
    "                if all_seqs[seg_idx][s] not in output_dict.keys():\n",
    "                    output_dict[all_seqs[seg_idx][s]] = 0\n",
    "                else:\n",
    "                    output_dict[all_seqs[seg_idx][s]] += 1\n",
    "                #print(\"start:\", tmp[output_dict[seqs[s]]], others[page][tmp[output_dict[seqs[s]]]])\n",
    "                idx = 0\n",
    "                for c in range(len(trans_dict_2[all_seqs[seg_idx][s]])):\n",
    "                    if trans_dict_2[all_seqs[seg_idx][s]][c] == '-':\n",
    "                        file.write('\\t')\n",
    "                    else:\n",
    "                        file.write(others[page][tmp[output_dict[all_seqs[seg_idx][s]]]+idx] + \"\\t\")\n",
    "                        idx += 1\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
